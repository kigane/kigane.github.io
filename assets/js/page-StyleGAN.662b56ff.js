(window.webpackJsonp=window.webpackJsonp||[]).push([[45],{488:function(t,a,e){t.exports=e.p+"assets/img/24c7a384fb06fd3edb6bb7903d916c336d571e9b691063e17640afca02ca23d7.8e3db9cc.png"},489:function(t,a,e){t.exports=e.p+"assets/img/ac49a1c8fc5cf5f07210cda1292f26555b0776ab260038d8ec59c0fd2effe641.7dff5a66.png"},490:function(t,a,e){t.exports=e.p+"assets/img/9eab363b61bedc12f965750ce7367d65d92c6e35d6606b3db8366a0c9663bc2d.91a0a39b.png"},598:function(t,a,e){"use strict";e.r(a);var s=e(1),r=Object(s.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h2",{attrs:{id:"stylegan-v1"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#stylegan-v1"}},[t._v("#")]),t._v(" StyleGAN v1")]),t._v(" "),s("h3",{attrs:{id:"摘要"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#摘要"}},[t._v("#")]),t._v(" 摘要")]),t._v(" "),s("p",[t._v("The "),s("code",[t._v("new architecture")]),t._v(" leads to an automatically learned, unsupervised "),s("code",[t._v("separation of high-level attributes")]),t._v(" (e.g., pose and identity when trained on human faces) and "),s("code",[t._v("stochastic variation")]),t._v(" in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably "),s("code",[t._v("better interpolation properties")]),t._v(", and also "),s("code",[t._v("better disentangles the latent factors of variation")]),t._v(". To "),s("code",[t._v("quantify interpolation quality and disentanglement, we propose two new, automated methods")]),t._v(" that are applicable to any generator architecture. Finally, we introduce "),s("code",[t._v("a new, highly varied and high-quality dataset of human faces")]),t._v(".")]),t._v(" "),s("h3",{attrs:{id:"图"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#图"}},[t._v("#")]),t._v(" 图")]),t._v(" "),s("p",[s("img",{attrs:{src:e(488),alt:"图 1"}})]),t._v(" "),s("h3",{attrs:{id:"要点"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#要点"}},[t._v("#")]),t._v(" 要点")]),t._v(" "),s("ul",[s("li",[t._v("MappingNetwork: 8层FC，z->w，Disentangle")]),t._v(" "),s("li",[t._v("AdaIN, Style w->ys,yb")]),t._v(" "),s("li",[t._v("G的初始层使用学到的参数，而非和w相关")]),t._v(" "),s("li",[t._v("在每个卷积层后，AdaIN前加入高斯噪音图像。并且每个通道各有一个可学习的缩放因子用于控制噪音")]),t._v(" "),s("li",[t._v("style mixing 选择一定比例的图像，在生成过程中随机选取某层，在该层后换掉原来的w，使用一个新的w'.一种正则技巧，不要让G觉得毗邻的style有关联")]),t._v(" "),s("li",[t._v("style影响全局的属性，如姿势，脸型等，noise影响更细节的地方。这和风格迁移领域的结论一致，即："),s("code",[t._v("空间不变的统计量(Gram矩阵，每通道像素的均值，方差等)编码了图片的style，随空间变化的特征编码了特定的实例")])]),t._v(" "),s("li",[t._v("Perceptual path length：用于量化隐空间的缠结程度。具体而言，用在隐空间插值生成的图像的变化程度来衡量。$ \\begin{array}  { l }  { l _ { z } = E [ \\frac { 1 } { \\epsilon ^ { 2 } } d ( G ( slerp ( z _ { 1 } , z _ { 2 } ; t ) ) _ { 1 } ) } ,{ G ( slerp  ( z _ { 1 } , z _ { 2 } ; t + \\epsilon ) ) ) ] } \\end{array} s.t. \\space t \\in [0, 1],\\epsilon=1e-4"),s("span",{staticClass:"katex"},[s("span",{staticClass:"katex-mathml"},[s("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[s("semantics",[s("mrow",[s("mtext",[t._v("，")]),s("mi",[t._v("d")]),s("mtext",[t._v("是感知距离。")])],1),s("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("，d是感知距离。")])],1)],1)],1),s("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[s("span",{staticClass:"base"},[s("span",{staticClass:"strut",staticStyle:{height:"0.6944em"}}),s("span",{staticClass:"mord cjk_fallback"},[t._v("，")]),s("span",{staticClass:"mord mathnormal"},[t._v("d")]),s("span",{staticClass:"mord cjk_fallback"},[t._v("是感知距离。")])])])]),t._v(" \\begin{array}  { l }  { l _ { w } = E [ \\frac { 1 } { \\epsilon ^ { 2 } } d ( G ( lerp ( z _ { 1 } , z _ { 2 } ; t ) ) _ { 1 } ) } ,{ G ( lerp  ( z _ { 1 } , z _ { 2 } ; t + \\epsilon ) ) ) ] } \\end{array} s.t. \\space t \\in [0, 1],\\epsilon=1e-4$")]),t._v(" "),s("li",[t._v("线性可分性: 为每个属性训练一个二分类SVM，然后计算条件熵H(Y|X)，值越小越好。#TODO")])]),t._v(" "),s("h2",{attrs:{id:"styleganv2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#styleganv2"}},[t._v("#")]),t._v(" StyleGANv2")]),t._v(" "),s("h3",{attrs:{id:"摘要-2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#摘要-2"}},[t._v("#")]),t._v(" 摘要")]),t._v(" "),s("p",[t._v("redesign the generator normalization"),s("br"),t._v("\nrevisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images."),s("br"),t._v("\nidentify a capacity problem")]),t._v(" "),s("h3",{attrs:{id:"图-2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#图-2"}},[t._v("#")]),t._v(" 图")]),t._v(" "),s("p",[s("img",{attrs:{src:e(489),alt:"图 3"}})]),t._v(" "),s("p",[s("img",{attrs:{src:e(490),alt:"图 2"}})]),t._v(" "),s("h3",{attrs:{id:"要点-2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#要点-2"}},[t._v("#")]),t._v(" 要点")])])}),[],!1,null,null,null);a.default=r.exports}}]);